{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(clf__C=[10, 1, 0.1, 0.01])\n",
    "grid_search = GridSearchCV(pipeline, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('bow',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        pre...\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1, param_grid={'clf__C': [10, 1, 0.1, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8927876966590066, Pipeline(memory=None,\n",
       "          steps=[('bow',\n",
       "                  CountVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                  input='content', lowercase=True, max_df=1.0,\n",
       "                                  max_features=None, min_df=1,\n",
       "                                  ngram_range=(1, 1), preprocessor=None,\n",
       "                                  stop_words=None, strip_accents=None,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, vocabulary=None)),\n",
       "                 ('clf',\n",
       "                  LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(C=1)),\n",
    "])\n",
    "pipeline.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8068242166755177"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.76      0.76      0.76       319\n",
      "           comp.graphics       0.68      0.76      0.72       389\n",
      " comp.os.ms-windows.misc       0.72      0.69      0.70       394\n",
      "comp.sys.ibm.pc.hardware       0.67      0.69      0.68       392\n",
      "   comp.sys.mac.hardware       0.76      0.81      0.78       385\n",
      "          comp.windows.x       0.83      0.70      0.76       395\n",
      "            misc.forsale       0.81      0.90      0.86       390\n",
      "               rec.autos       0.84      0.86      0.85       396\n",
      "         rec.motorcycles       0.94      0.93      0.94       398\n",
      "      rec.sport.baseball       0.86      0.90      0.88       397\n",
      "        rec.sport.hockey       0.93      0.93      0.93       399\n",
      "               sci.crypt       0.93      0.88      0.91       396\n",
      "         sci.electronics       0.70      0.74      0.72       393\n",
      "                 sci.med       0.86      0.78      0.82       396\n",
      "               sci.space       0.92      0.90      0.91       394\n",
      "  soc.religion.christian       0.82      0.92      0.87       398\n",
      "      talk.politics.guns       0.74      0.88      0.80       364\n",
      "   talk.politics.mideast       0.93      0.82      0.87       376\n",
      "      talk.politics.misc       0.72      0.55      0.62       310\n",
      "      talk.religion.misc       0.66      0.61      0.64       251\n",
      "\n",
      "                accuracy                           0.81      7532\n",
      "               macro avg       0.80      0.80      0.80      7532\n",
      "            weighted avg       0.81      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(classification_report(test[\"target\"], predictions, target_names=test[\"target_names\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tfidf = Pipeline([\n",
    "    ('bow', TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    ngram_range=(1,2), \n",
    "                    vocabulary=None,\n",
    "                    max_df=1.0, \n",
    "                    stop_words={'english'},\n",
    "                    smooth_idf=True\n",
    "                    )),\n",
    "    ('clf', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(clf__C=[10, 1, 0.1, 0.01])\n",
    "grid_search_tfidf = GridSearchCV(pipeline_tfidf, params, scoring=\"accuracy\", cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('bow',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 2),\n",
       "                                                        n...\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='warn',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1, param_grid={'clf__C': [10, 1, 0.1, 0.01]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tfidf.fit(train[\"data\"], train[\"target\"], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9192151316952448, Pipeline(memory=None,\n",
       "          steps=[('bow',\n",
       "                  TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                  decode_error='strict',\n",
       "                                  dtype=<class 'numpy.float64'>,\n",
       "                                  encoding='utf-8', input='content',\n",
       "                                  lowercase=True, max_df=1.0, max_features=None,\n",
       "                                  min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                  preprocessor=None, smooth_idf=True,\n",
       "                                  stop_words={'english'}, strip_accents=None,\n",
       "                                  sublinear_tf=False,\n",
       "                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                  tokenizer=None, use_idf=True,\n",
       "                                  vocabulary=None)),\n",
       "                 ('clf',\n",
       "                  LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                     fit_intercept=True, intercept_scaling=1,\n",
       "                                     l1_ratio=None, max_iter=100,\n",
       "                                     multi_class='warn', n_jobs=None,\n",
       "                                     penalty='l2', random_state=None,\n",
       "                                     solver='warn', tol=0.0001, verbose=0,\n",
       "                                     warm_start=False))],\n",
       "          verbose=False))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tfidf.best_score_, grid_search_tfidf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_tfidf = Pipeline([\n",
    "    ('bow', TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    ngram_range=(1,2), \n",
    "                    vocabulary=None,\n",
    "                    max_df=1.0, \n",
    "                    smooth_idf=True\n",
    "                    )),\n",
    "    \n",
    "    ('clf', LogisticRegression(C=10)),])\n",
    "pipeline_tfidf.fit(train[\"data\"], train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.846388741370154"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline_tfidf.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# контрастные соединения\n",
    "contrast_conj = set([\n",
    "'alternatively','anyway','but','by contrast','differ from','elsewhere','even so','however','in contrast','in fact',\n",
    "'in other respects','in spite of','in that respect','instead','nevertheless','on the contrary','on the other hand',\n",
    "'rather','though','whereas','yet'])\n",
    "\n",
    "# чтобы получить \"чистоту\" отзыва ~ показывает ту же тональность к обзору (~ 1) или изменение настроения (~ 0)\n",
    "def purity(sentences):\n",
    "    # получает полярность по всем предложениям\n",
    "    polarities = np.array([TextBlob(x).sentiment.polarity for x in sentences])\n",
    "    return polarities.sum() / np.abs(polarities).sum()\n",
    "\n",
    "# uppercase pattern\n",
    "uppercase_pattern = re.compile(r'(\\b[0-9]*[A-Z]+[0-9]*[A-Z]+[0-9]*\\b)')\n",
    "\n",
    "# регулярное выражение для разделения отзыва на предложения, вы можете использовать метод из textblob: TextBlob(x).sentences_\n",
    "sentence_splitter = re.compile('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\!|\\?|\\.)\\s')\n",
    "# you can https://regex101.com/ for regex creation/checking, very convenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Subject=train['data'][1].split('Subject:')[1].split('\\n')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_smiles = set([\n",
    "\":‑)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‑D\",\":D\",\"8‑D\",\"8D\",\n",
    "\"x‑D\",\"xD\",\"X‑D\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‑)\",\";)\",\"*-)\",\"*)\",\";‑]\",\";]\",\";^)\",\":‑,\",\";D\",\":‑P\",\":P\",\"X‑P\",\"XP\",\n",
    "\"x‑p\",\"xp\",\":‑p\",\":p\",\":‑Þ\",\":Þ\",\":‑þ\",\":þ\",\":‑b\",\":b\",\"d:\",\"=p\",\">:P\", \":'‑)\", \":')\",  \":-*\", \":*\", \":×\"\n",
    "])\n",
    "negative_smiles = set([\n",
    "\":‑(\",\":(\",\":‑c\",\":c\",\":‑<\",\":<\",\":‑[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‑':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‑/\",\n",
    "\":/\",\":‑.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‑|\",\":|\",\"|‑O\",\"<:‑|\"\n",
    "])\n",
    "\n",
    "def get_token_level_features(texts, visualize=True):\n",
    "\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = texts # this is our review\n",
    "    \n",
    "    tdf['subject'] = tdf.text.apply(lambda s: s.split('Subject:')[1].split('\\n')[0])\n",
    "    tdf['imail_cnt'] = tdf.text.apply(lambda s: len([x for x in s.split('@')]))\n",
    " \n",
    "    tdf['positive_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in positive_smiles]))\n",
    "    tdf['negative_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in negative_smiles]))\n",
    "    \n",
    "    if visualize:\n",
    "        # this is used for visual clarity, return pd.DataFrame\n",
    "        return tdf \n",
    "    else:\n",
    "        # get correct (and sparse) representation of feature matrix F\n",
    "        from scipy.sparse import csr_matrix \n",
    "        return csr_matrix(tdf[tdf.columns[2:]].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 5)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features=get_token_level_features(train['data'])\n",
    "df_features[['subject']] = df_features[['subject']].fillna('NoSubject')\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    ngram_range=(1,2), \n",
    "                    vocabulary=None,\n",
    "                    max_df=1.0, \n",
    "                    smooth_idf=True\n",
    "                    )\n",
    "\n",
    "t2=TfidfVectorizer(\n",
    "                    analyzer='word',\n",
    "                    ngram_range=(1,2), \n",
    "                    vocabulary=None,\n",
    "                    max_df=1.0, \n",
    "                    smooth_idf=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_fea=t1.fit_transform(train[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subject_fea=t2.fit_transform(df_features[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x1181803 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4696188 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x24774 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 113711 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_fea=get_token_level_features(train['data'], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x3 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 11762 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "train_fea=hstack((core_fea, subject_fea, add_fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x1206580 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4821661 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg=LogisticRegression(C=10, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=100,\n",
    "                                    multi_class='warn', n_jobs=-1,\n",
    "                                    penalty='l2', random_state=None,\n",
    "                                    solver='warn', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logReg.fit(train_fea, train[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_test=get_token_level_features(test['data'])\n",
    "df_features_test[['subject']] = df_features_test[['subject']].fillna('NoSubject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_core = t1.transform(test[\"data\"])\n",
    "test_subject_fea = t2.transform(df_features_test[\"subject\"])\n",
    "add_fea_test = get_token_level_features(test['data'], False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_features=hstack((test_core, test_subject_fea, add_fea_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355018587360595"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = logReg.predict(test_features)\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hyperopt import hp, tpe, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "import lightgbm as lgb\n",
    "import hyperopt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_class':20,\n",
    "    'max_depth': 4,\n",
    "    'num_leaves': 20,\n",
    "    'verbose': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgbm.LGBMClassifier(\n",
    "        **params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(core_fea, train[\"target\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalid_0's multi_logloss: 0.589869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.2, max_depth=5,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=10,\n",
       "               objective='multiclass', random_state=None, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model.fit(X_train, y_train, eval_set=[(X_test, y_test)],verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615507169410515"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions =lgb_model.predict(test_core)\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355650776647028"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions =lgb_model.predict(X_test)\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train=lgb.Dataset(core_fea, label=train[\"target\"], params={'verbose': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = lgb.cv(params, d_train, num_boost_round=1000, nfold=5, \n",
    "                     early_stopping_rounds=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'objective': 'mlogloss', \n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=250,\n",
    "        learning_rate=0.05,\n",
    "        n_jobs=4,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, core_fea, train[\"target\"], scoring='accuracy', cv=StratifiedKFold()).mean()\n",
    "    print(\"Accuracy {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                           | 0/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:19:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:17:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[23:19:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.822 params {'max_depth': 7, 'gamma': '0.245', 'colsample_bytree': '0.671'}                                                                                  \n",
      " 10%|█████████                                                                                  | 1/10 [2:59:06<26:51:54, 10746.09s/trial, best loss: 0.82190210037961]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:18:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:54:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[01:27:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.839 params {'max_depth': 7, 'gamma': '0.183', 'colsample_bytree': '0.357'}                                                                                  \n",
      " 20%|██████████████████▍                                                                         | 2/10 [4:40:47<20:47:02, 9352.82s/trial, best loss: 0.82190210037961]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:59:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[02:34:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[03:09:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.826 params {'max_depth': 5, 'gamma': '0.328', 'colsample_bytree': '0.535'}                                                                                  \n",
      " 30%|███████████████████████████▌                                                                | 3/10 [6:26:05<16:24:55, 8442.23s/trial, best loss: 0.82190210037961]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:45:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[04:31:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[05:18:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.827 params {'max_depth': 8, 'gamma': '0.046', 'colsample_bytree': '0.531'}                                                                                  \n",
      " 40%|████████████████████████████████████▊                                                       | 4/10 [8:46:29<14:03:40, 8436.73s/trial, best loss: 0.82190210037961]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:05:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[06:42:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[07:19:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.823 params {'max_depth': 5, 'gamma': '0.456', 'colsample_bytree': '0.583'}                                                                                  \n",
      " 50%|█████████████████████████████████████████████▌                                             | 5/10 [10:38:11<10:59:41, 7916.25s/trial, best loss: 0.82190210037961]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07:57:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[09:00:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:06:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.816 params {'max_depth': 6, 'gamma': '0.325', 'colsample_bytree': '0.972'}                                                                                  \n",
      " 60%|█████████████████████████████████████████████████████▍                                   | 6/10 [13:56:25<10:07:18, 9109.72s/trial, best loss: 0.8156265032564001]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:15:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[11:58:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[12:41:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.819 params {'max_depth': 4, 'gamma': '0.113', 'colsample_bytree': '0.791'}                                                                                  \n",
      " 70%|██████████████████████████████████████████████████████████████▉                           | 7/10 [16:04:35<7:14:10, 8683.64s/trial, best loss: 0.8156265032564001]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:23:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:45:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.800 params {'max_depth': 2, 'gamma': '0.120', 'colsample_bytree': '0.488'}                                                                                  \n",
      " 80%|████████████████████████████████████████████████████████████████████████                  | 8/10 [17:14:25<4:04:31, 7335.55s/trial, best loss: 0.7998946732654342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:33:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:55:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[15:17:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.821 params {'max_depth': 3, 'gamma': '0.257', 'colsample_bytree': '0.312'}                                                                                  \n",
      " 90%|█████████████████████████████████████████████████████████████████████████████████         | 9/10 [18:20:58<1:45:32, 6332.83s/trial, best loss: 0.7998946732654342]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n",
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:40:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:20:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[16:58:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy 0.813 params {'max_depth': 3, 'gamma': '0.032', 'colsample_bytree': '0.819'}                                                                                  \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 10/10 [20:16:28<00:00, 7298.83s/trial, best loss: 0.7998946732654342]\n"
     ]
    }
   ],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperopt estimated optimum {'colsample_bytree': 0.4876012032299183, 'gamma': 0.11966423607931742, 'max_depth': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best={\n",
    "        'objective': 'multi:softmax', 'num_class': 20, \n",
    "        'max_depth': 7, 'gamma': 0.183, 'colsample_bytree': 0.357}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xgb=xgb.DMatrix(X_train, label=y_train)\n",
    "valid_xgb=xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:12:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softmax' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\ttrain-mlogloss:1.95572\tvalid-mlogloss:2.03817\n",
      "[1]\ttrain-mlogloss:1.63448\tvalid-mlogloss:1.79574\n",
      "[2]\ttrain-mlogloss:1.39261\tvalid-mlogloss:1.61236\n",
      "[3]\ttrain-mlogloss:1.20784\tvalid-mlogloss:1.46787\n",
      "[4]\ttrain-mlogloss:1.05931\tvalid-mlogloss:1.35493\n",
      "[5]\ttrain-mlogloss:0.94170\tvalid-mlogloss:1.26359\n",
      "[6]\ttrain-mlogloss:0.84989\tvalid-mlogloss:1.18817\n",
      "[7]\ttrain-mlogloss:0.76977\tvalid-mlogloss:1.12874\n",
      "[8]\ttrain-mlogloss:0.70081\tvalid-mlogloss:1.08433\n",
      "[9]\ttrain-mlogloss:0.64087\tvalid-mlogloss:1.03942\n",
      "[10]\ttrain-mlogloss:0.58532\tvalid-mlogloss:1.00153\n",
      "[11]\ttrain-mlogloss:0.53813\tvalid-mlogloss:0.96786\n",
      "[12]\ttrain-mlogloss:0.49616\tvalid-mlogloss:0.94235\n",
      "[13]\ttrain-mlogloss:0.45837\tvalid-mlogloss:0.91686\n",
      "[14]\ttrain-mlogloss:0.42087\tvalid-mlogloss:0.88829\n",
      "[15]\ttrain-mlogloss:0.38652\tvalid-mlogloss:0.86445\n",
      "[16]\ttrain-mlogloss:0.35796\tvalid-mlogloss:0.84325\n",
      "[17]\ttrain-mlogloss:0.33237\tvalid-mlogloss:0.82477\n",
      "[18]\ttrain-mlogloss:0.31125\tvalid-mlogloss:0.80840\n",
      "[19]\ttrain-mlogloss:0.28955\tvalid-mlogloss:0.79366\n",
      "[20]\ttrain-mlogloss:0.27054\tvalid-mlogloss:0.78006\n",
      "[21]\ttrain-mlogloss:0.25099\tvalid-mlogloss:0.77039\n",
      "[22]\ttrain-mlogloss:0.23367\tvalid-mlogloss:0.76029\n",
      "[23]\ttrain-mlogloss:0.21904\tvalid-mlogloss:0.75028\n",
      "[24]\ttrain-mlogloss:0.20512\tvalid-mlogloss:0.74131\n",
      "[25]\ttrain-mlogloss:0.19304\tvalid-mlogloss:0.73229\n",
      "[26]\ttrain-mlogloss:0.18237\tvalid-mlogloss:0.72444\n",
      "[27]\ttrain-mlogloss:0.17202\tvalid-mlogloss:0.71661\n",
      "[28]\ttrain-mlogloss:0.16294\tvalid-mlogloss:0.71113\n",
      "[29]\ttrain-mlogloss:0.15368\tvalid-mlogloss:0.70335\n",
      "[30]\ttrain-mlogloss:0.14480\tvalid-mlogloss:0.69696\n",
      "[31]\ttrain-mlogloss:0.13771\tvalid-mlogloss:0.69269\n",
      "[32]\ttrain-mlogloss:0.13099\tvalid-mlogloss:0.68615\n",
      "[33]\ttrain-mlogloss:0.12440\tvalid-mlogloss:0.67947\n",
      "[34]\ttrain-mlogloss:0.11870\tvalid-mlogloss:0.67553\n",
      "[35]\ttrain-mlogloss:0.11324\tvalid-mlogloss:0.67050\n",
      "[36]\ttrain-mlogloss:0.10832\tvalid-mlogloss:0.66604\n",
      "[37]\ttrain-mlogloss:0.10370\tvalid-mlogloss:0.66022\n",
      "[38]\ttrain-mlogloss:0.09894\tvalid-mlogloss:0.65773\n",
      "[39]\ttrain-mlogloss:0.09435\tvalid-mlogloss:0.65443\n",
      "[40]\ttrain-mlogloss:0.08963\tvalid-mlogloss:0.65134\n",
      "[41]\ttrain-mlogloss:0.08575\tvalid-mlogloss:0.64749\n",
      "[42]\ttrain-mlogloss:0.08234\tvalid-mlogloss:0.64288\n",
      "[43]\ttrain-mlogloss:0.07910\tvalid-mlogloss:0.63945\n",
      "[44]\ttrain-mlogloss:0.07583\tvalid-mlogloss:0.63620\n",
      "[45]\ttrain-mlogloss:0.07309\tvalid-mlogloss:0.63372\n",
      "[46]\ttrain-mlogloss:0.07015\tvalid-mlogloss:0.63135\n",
      "[47]\ttrain-mlogloss:0.06722\tvalid-mlogloss:0.62936\n",
      "[48]\ttrain-mlogloss:0.06472\tvalid-mlogloss:0.62667\n",
      "[49]\ttrain-mlogloss:0.06201\tvalid-mlogloss:0.62501\n",
      "[50]\ttrain-mlogloss:0.05973\tvalid-mlogloss:0.62321\n",
      "[51]\ttrain-mlogloss:0.05771\tvalid-mlogloss:0.62128\n",
      "[52]\ttrain-mlogloss:0.05583\tvalid-mlogloss:0.61905\n",
      "[53]\ttrain-mlogloss:0.05406\tvalid-mlogloss:0.61730\n",
      "[54]\ttrain-mlogloss:0.05231\tvalid-mlogloss:0.61522\n",
      "[55]\ttrain-mlogloss:0.05098\tvalid-mlogloss:0.61360\n",
      "[56]\ttrain-mlogloss:0.04942\tvalid-mlogloss:0.61188\n",
      "[57]\ttrain-mlogloss:0.04798\tvalid-mlogloss:0.61088\n",
      "[58]\ttrain-mlogloss:0.04668\tvalid-mlogloss:0.60942\n",
      "[59]\ttrain-mlogloss:0.04553\tvalid-mlogloss:0.60824\n",
      "[60]\ttrain-mlogloss:0.04432\tvalid-mlogloss:0.60678\n",
      "[61]\ttrain-mlogloss:0.04329\tvalid-mlogloss:0.60551\n",
      "[62]\ttrain-mlogloss:0.04239\tvalid-mlogloss:0.60442\n",
      "[63]\ttrain-mlogloss:0.04136\tvalid-mlogloss:0.60349\n",
      "[64]\ttrain-mlogloss:0.04036\tvalid-mlogloss:0.60274\n",
      "[65]\ttrain-mlogloss:0.03949\tvalid-mlogloss:0.60156\n",
      "[66]\ttrain-mlogloss:0.03876\tvalid-mlogloss:0.60078\n",
      "[67]\ttrain-mlogloss:0.03801\tvalid-mlogloss:0.60013\n",
      "[68]\ttrain-mlogloss:0.03742\tvalid-mlogloss:0.59926\n",
      "[69]\ttrain-mlogloss:0.03677\tvalid-mlogloss:0.59901\n",
      "[70]\ttrain-mlogloss:0.03613\tvalid-mlogloss:0.59833\n",
      "[71]\ttrain-mlogloss:0.03545\tvalid-mlogloss:0.59722\n",
      "[72]\ttrain-mlogloss:0.03485\tvalid-mlogloss:0.59690\n",
      "[73]\ttrain-mlogloss:0.03438\tvalid-mlogloss:0.59649\n",
      "[74]\ttrain-mlogloss:0.03389\tvalid-mlogloss:0.59619\n",
      "[75]\ttrain-mlogloss:0.03356\tvalid-mlogloss:0.59578\n",
      "[76]\ttrain-mlogloss:0.03321\tvalid-mlogloss:0.59567\n",
      "[77]\ttrain-mlogloss:0.03278\tvalid-mlogloss:0.59513\n",
      "[78]\ttrain-mlogloss:0.03248\tvalid-mlogloss:0.59470\n",
      "[79]\ttrain-mlogloss:0.03216\tvalid-mlogloss:0.59434\n",
      "[80]\ttrain-mlogloss:0.03187\tvalid-mlogloss:0.59398\n",
      "[81]\ttrain-mlogloss:0.03153\tvalid-mlogloss:0.59334\n",
      "[82]\ttrain-mlogloss:0.03117\tvalid-mlogloss:0.59287\n",
      "[83]\ttrain-mlogloss:0.03100\tvalid-mlogloss:0.59267\n",
      "[84]\ttrain-mlogloss:0.03091\tvalid-mlogloss:0.59237\n",
      "[85]\ttrain-mlogloss:0.03068\tvalid-mlogloss:0.59220\n",
      "[86]\ttrain-mlogloss:0.03038\tvalid-mlogloss:0.59204\n",
      "[87]\ttrain-mlogloss:0.03009\tvalid-mlogloss:0.59192\n",
      "[88]\ttrain-mlogloss:0.02984\tvalid-mlogloss:0.59159\n",
      "[89]\ttrain-mlogloss:0.02961\tvalid-mlogloss:0.59117\n",
      "[90]\ttrain-mlogloss:0.02940\tvalid-mlogloss:0.59086\n",
      "[91]\ttrain-mlogloss:0.02922\tvalid-mlogloss:0.59052\n",
      "[92]\ttrain-mlogloss:0.02911\tvalid-mlogloss:0.59061\n",
      "[93]\ttrain-mlogloss:0.02896\tvalid-mlogloss:0.59052\n",
      "[94]\ttrain-mlogloss:0.02882\tvalid-mlogloss:0.59064\n",
      "[95]\ttrain-mlogloss:0.02865\tvalid-mlogloss:0.59043\n",
      "[96]\ttrain-mlogloss:0.02840\tvalid-mlogloss:0.59007\n",
      "[97]\ttrain-mlogloss:0.02830\tvalid-mlogloss:0.58989\n",
      "[98]\ttrain-mlogloss:0.02813\tvalid-mlogloss:0.59004\n",
      "[99]\ttrain-mlogloss:0.02802\tvalid-mlogloss:0.58987\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(params_best, data_xgb, num_boost_round=100, evals=[(data_xgb, 'train'), (valid_xgb, 'valid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_xgb=xgb.DMatrix(test_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7537174721189591"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions =xgb_model.predict(test_xgb)\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool, CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {'iterations':1000,\n",
    "    'learning_rate' :0.01,\n",
    "    'random_strength' : 0.1,\n",
    "    'depth':3,\n",
    "    'loss_function' :'MultiClass',\n",
    "    'eval_metric':'Accuracy',\n",
    "    'leaf_estimation_method': 'Newton'\n",
    "    }\n",
    "    \n",
    "    clf = CatBoostClassifier(\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, core_fea, train[\"target\"], scoring='accuracy', cv=StratifiedKFold()).mean()\n",
    "    print(\"Accuracy {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.05, 1.0),\n",
    "    'random_strength': hp.uniform('random_strength', 0.1, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                            | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lost\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:629: FutureWarning: The default value of n_split will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(NSPLIT_WARNING, FutureWarning)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'iterations':1000,\n",
    "    'learning_rate' :0.01,\n",
    "    'random_strength' : 0.1,\n",
    "    'depth':3,\n",
    "    'loss_function' :'MultiClass',\n",
    "    'eval_metric':'Accuracy',\n",
    "    'leaf_estimation_method': 'Newton'\n",
    "    }\n",
    "    \n",
    "clf = CatBoostClassifier(\n",
    "        **params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(X_train, label=y_train)\n",
    "valid_pool = Pool(X_test, label=y_test) \n",
    "test_pool = Pool(test_core) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-07eb7d18e3e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_pool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_pool' is not defined"
     ]
    }
   ],
   "source": [
    "clf.fit(train_pool,eval_set=valid_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions =clf.predict(test_pool)\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
